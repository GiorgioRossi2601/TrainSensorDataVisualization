# Definition of the custom network for container communication
networks:
  trains_network:
    driver: bridge  # Use the bridge network driver for isolated container communication

services:
  # Zookeeper service for Kafka coordination
  zookeeper:
    image: confluentinc/cp-zookeeper:latest  # Latest Zookeeper image from Confluent
    container_name: zookeeper  # Container name for easier identification
    hostname: zookeeper  # Hostname for internal communication within the network
    environment:
      ZOOKEEPER_SERVER_ID: 1  # Unique server ID for the Zookeeper instance
      ZOOKEEPER_CLIENT_PORT: 2181  # Port for client connections to Zookeeper
      ZOOKEEPER_TICK_TIME: 2000  # Time unit (in milliseconds) used by Zookeeper for internal operations
    ports:
      - "2181:2181"  # Expose port 2181 to allow external clients to connect
    networks:
      - trains_network  # Connect Zookeeper to the custom Docker network

  # Kafka service for message handling
  kafka:
    image: confluentinc/cp-kafka:latest  # Latest Kafka image from Confluent
    container_name: kafka  # Container name for easier identification
    hostname: kafka  # Hostname for internal communication within the network
    depends_on:
      - zookeeper  # Ensure Kafka starts only after Zookeeper is running
    environment:
      KAFKA_BROKER_ID: 1  # Unique identifier for this Kafka broker
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181  # Address to connect to Zookeeper
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9092,OUTSIDE://localhost:19092  # Listeners for internal and external communication
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9092,OUTSIDE://0.0.0.0:19092  # Ports to bind Kafka to for listening
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE  # Listener for internal Kafka broker communication
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT  # Security protocol mappings for each listener
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1  # Replication factor for internal Kafka topics
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'  # Automatically create topics when required
    ports:
      - "9092:9092"  # Expose Kafka's internal communication port
      - "19092:19092"  # Expose Kafka's external communication port
    networks:
      - trains_network  # Connect Kafka to the custom Docker network
    healthcheck:
      test: [ "CMD", "nc", "-z", "localhost", "9092" ]  # Command to check if Kafka is running
      interval: 10s  # Time interval between health checks
      timeout: 5s  # Maximum time to wait for the health check
      retries: 5  # Retry count before reporting the container as unhealthy

  # Data sensor simulator as Kafka producer
  data_sensor_simulator_kafka_producer:
    build:
      context: ./kafka_producer  # Path to the producer's Dockerfile
      dockerfile: Dockerfile  # Specify the Dockerfile for building the image
    container_name: kafka_producer  # Container name for easier identification
    hostname: producer  # Hostname for internal communication
    environment:
      KAFKA_BROKER: 'kafka:9092'  # Kafka broker address for the producer to connect to
      TOPIC_NAME: "train-sensor-data"  # Kafka topic to publish sensor data
    depends_on:
      - kafka  # Ensure this producer starts only after Kafka is running
    networks:
      - trains_network  # Connect the producer to the custom Docker network
    entrypoint: ["/wait-for-it.sh", "kafka:9092", "--", "sh", "-c", "python data_simulator.py"]  # Wait for Kafka to start before executing the script

  # Flask application as Kafka consumer
  flask_app_kafka_consumer:
    build:
      context: ./kafka_flask_consumer  # Path to the consumer's Dockerfile
      dockerfile: Dockerfile  # Specify the Dockerfile for building the image
    container_name: kafka_flask_consumer  # Container name for easier identification
    hostname: consumer  # Hostname for internal communication
    environment:
      KAFKA_BROKER: 'kafka:9092'  # Kafka broker address for the consumer to connect to
      TOPIC_NAME: 'train-sensor-data'  # Kafka topic to consume sensor data
    ports:
      - "5000:5000"  # Expose Flask's port for external access
    depends_on:
      - kafka  # Ensure the consumer starts only after Kafka is running
    networks:
      - trains_network  # Connect the consumer to the custom Docker network
    entrypoint: ["/wait-for-it.sh", "kafka:9092", "--", "sh", "-c", "flask run"]  # Wait for Kafka to start before executing the Flask server

  # Producer and Consumer pairs for the first vehicles
  # Producer1
  synthetic_data_sensor_generator_kafka_producer:
    build:
      context: ./real_kafka_producer  # Path to the producer's Dockerfile
      dockerfile: Dockerfile  # Specify the Dockerfile for building the image
    container_name: producer_1  # Container name for easier identification
    hostname: producer_1  # Hostname for internal communication
    environment:
      KAFKA_BROKER: 'kafka:9092'  # Kafka broker address
      VEHICLE_NAME: 'e700_4801'  # Name of the vehicle this producer generates data for
    depends_on:
      - kafka  # Ensure this producer starts only after Kafka is running
    networks:
      - trains_network  # Connect the producer to the custom Docker network
    entrypoint: ["/wait-for-it.sh", "kafka:9092", "--", "sh", "-c", "python synthetic_data_generator.py"]  # Wait for Kafka before running the script

  # Consumer1
  real_data_kafka_consumer:
    build:
      context: ./real_kafka_consumer  # Path to the consumer's Dockerfile
      dockerfile: Dockerfile  # Specify the Dockerfile for building the image
    container_name: consumer_1  # Container name for easier identification
    hostname: consumer_1  # Hostname for internal communication
    environment:
      KAFKA_BROKER: 'kafka:9092'  # Kafka broker address
      VEHICLE_NAME: 'e700_4801'  # Name of the vehicle this consumer processes data for
    depends_on:
      - kafka  # Ensure this consumer starts only after Kafka is running
    networks:
      - trains_network  # Connect the consumer to the custom Docker network
    entrypoint: ["/wait-for-it.sh", "kafka:9092", "--", "sh", "-c", "python consumer_synthetic_data.py"]  # Wait for Kafka before running the script

  # Producer and Consumer pairs for the second vehicles
  # Producer2
  producer_2:
    build:
      context: ./real_kafka_producer
    container_name: producer_2
    hostname: producer_2
    environment:
      KAFKA_BROKER: 'kafka:9092'
      VEHICLE_NAME: 'e700_4802'  # Name of the second vehicle
    depends_on:
      - kafka
    networks:
      - trains_network
    entrypoint: ["/wait-for-it.sh", "kafka:9092", "--", "sh", "-c", "python synthetic_data_generator.py"]

  # Consumer2
  consumer_2:
    build:
      context: ./real_kafka_consumer
    container_name: consumer_2
    hostname: consumer_2
    environment:
      KAFKA_BROKER: 'kafka:9092'
      VEHICLE_NAME: 'e700_4802'  # Name of the second vehicle
    depends_on:
      - kafka
    networks:
      - trains_network
    entrypoint: ["/wait-for-it.sh", "kafka:9092", "--", "sh", "-c", "python consumer_synthetic_data.py"]

  # Producer and Consumer pairs for the third vehicles
  # Producer3
  producer_3:
    build:
      context: ./real_kafka_producer
    container_name: producer_3
    hostname: producer_3
    environment:
      KAFKA_BROKER: 'kafka:9092'
      VEHICLE_NAME: 'e700_4803'  # Name of the third vehicle
    depends_on:
      - kafka
    networks:
      - trains_network
    entrypoint: ["/wait-for-it.sh", "kafka:9092", "--", "sh", "-c", "python synthetic_data_generator.py"]

  # Consumer3
  consumer_3:
    build:
      context: ./real_kafka_consumer
    container_name: consumer_3
    hostname: consumer_3
    environment:
      KAFKA_BROKER: 'kafka:9092'
      VEHICLE_NAME: 'e700_4803'  # Name of the third vehicle
    depends_on:
      - kafka
    networks:
      - trains_network
    entrypoint: ["/wait-for-it.sh", "kafka:9092", "--", "sh", "-c", "python consumer_synthetic_data.py"]
